{
    "group": "默认书源",
    "bookSource": [],
    "pySource": [
        {
            "source": "shengxuxiaoshuowang",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # POST请求的URL\n    url = 'http://www.shengxuxu.net/search.html?searchtype=novelname&searchkey='\n    # POST请求的数据\n    data = {'searchkey': query.encode('utf-8'), \"searchtype\": 'novelname'}\n    # 发送get请求\n    response = requests.get(url, params=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".librarylist > li\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\".novelname\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\".novelname\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\".info span\")[1].text.replace(\"作者：\",\"\"),\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    # document.querySelectorAll(\".dirlist\")[1].querySelectorAll(\"a\")[1664]\n    for el in soup.select(\".dirlist\")[1].select(\"a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\"#chaptercontent\")[0].prettify()\n    return content\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "明智屋",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    query = query.encode('utf-8')\n    # url encode\n    query = quote(query)\n    url = f'https://www.mingzw.net/mzwlist/{query}.html'\n    # 发送get请求\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request.url)\n    log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".figure-horizontal\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].attrs[\"title\"],\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\"dd\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\".novel-name\")[0].text.replace(\"《\", \"\").replace(\"》\", \"\")\n    # 作者\n    info[\"author\"] = soup.select(\".picinfo a\")[0].text\n    # 简介\n    info[\"intro\"] = soup.select(\".content\")[0].text\n    # 封面\n    info[\"cover\"] = urljoin(book_url, soup.select(\".pic img\")[0].get(\"src\"))\n    # 章节列表\n    info[\"chapterList\"] = []\n    chapterUrl = soup.select(\".view-all-btn\")[0].get(\"href\")\n    parseChapterList(book_url, urljoin(book_url, chapterUrl), info[\"chapterList\"])\n\n    return info\n\n\ndef parseChapterList(book_url, chapterUrl, chapterList):\n    response = requests.get(chapterUrl, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 递归加载章节列表\n    dt = 0\n    for el in soup.select(\".content a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(chapterUrl, el.get(\"href\"))\n        })\n\n    chapterList.pop(-1)\n    chapterList.pop(-1)\n    chapterList.pop(-1)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".contents\")[0].prettify()\n    content = content[:content.rindex(\"推荐小说:\")]\n    return content\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "书趣阁",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    # query = query.encode('utf-8')\n    # url encode\n    # query = quote(query)\n    url = f'https://www.22shuqu.com/search/'\n    # searchkey=%E6%88%91%E7%9A%84&Submit=\n    # data = {\"searchkey\": query, \"Submit\": \"\"}\n    data = f'searchkey={quote(query)}&Submit={quote(\"搜索\")}'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # 发送get请求\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for i,bookEl in enumerate(soup.select(\".txt-list > li \")):\n        if i == 0:\n            continue\n        books.append({\n            \"bookTitle\": bookEl.select(\".s2 > a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\".s2 > a\")[0].get(\"href\"))\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n\n    while True:\n\n        for el in soup.select(\".section-list\")[1].select(\"a\"):\n            info[\"chapterList\"].append({\n                \"title\": el.text,\n                \"url\": urljoin(book_url, el.get(\"href\"))\n            })\n        if soup.select(\".index-container-btn\")[1].text.strip() == \"下一页\":\n            soup = BeautifulSoup(requests.get(urljoin(book_url, soup.select(\".index-container-btn\")[1].get(\"href\")), timeout=(5, 10)).text, 'html.parser')\n        else:\n            break\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\"#content\")[0].prettify()\n\n    if soup.select(\"#next_url\")[0].text.strip() == \"下一页\":\n        content += getChapterContent(urljoin(chapter_url, soup.select(\"#next_url\")[0].get(\"href\")))\n    return content\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "起点",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    # query = query.encode('utf-8')\n    # url encode\n    query = quote(query)\n    url = f\"https://m.qidian.com/soushu/{query}.html\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    log(response.text)\n    # 创建BeautifulSoup对象 .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0]\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".y-list__item\"):\n        bid = bookEl.select(\"a\")[0].attrs[\"data-bid\"]\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].attrs[\"title\"].replace(\"在线阅读\", \"\"),\n            \"bookUrl\": f\"https://m.qidian.com/book/{bid}/\",\n            \"bookAuthor\": bookEl.select(\"div\")[2].select(\"p\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    log(response.request)\n    log(response.text)\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    log(\"加载章节目录\")\n    chapterUrl = urljoin(book_url, soup.select(\"#details-menu\")[0].attrs[\"href\"])\n    response = requests.get(chapterUrl, headers=headers, timeout=(5, 10))\n    log(response.request)\n    response.encoding = \"utf-8\"\n    log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    for el in soup.select(\".y-list__content .y-list__item a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.select(\"h2\")[0].text.replace(\"\\xa0\", \" \"),\n            \"url\": \"https://m.qidian.com\" + el.attrs[\"href\"]\n        })\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    log(response.request)\n    log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".jsChapterWrapper > div\")[0].prettify()\n    return content\n\n\n",
            "version": 7,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "22笔趣阁",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    # query = query.encode('utf-8')\n    # url encode\n    url = f\"https://www.22biqu.com/ss/\"\n    # POST请求的数据\n    data = f'searchkey={quote(query)}&Submit={quote(\"搜索\")}'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # 发送POST请求Content-Type:\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    # 创建BeautifulSoup对象 .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0] document.querySelectorAll(\".txt-list > li\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n\n    for i, bookEl in enumerate(soup.select(\".txt-list > li\")):\n        if i == 0:\n            continue\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].attrs[\"href\"]),\n            \"bookAuthor\": bookEl.select(\".s4\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # log(response.request)\n    # log(response.text)\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    loadChapterList(book_url, soup, info[\"chapterList\"])\n    return info\n\n\ndef loadChapterList(book_url, soup, chapterList):\n    log(\"加载章节目录\")\n    for el in soup.select(\".section-list\")[1].select(\"a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n    if soup.select(\".index-container-btn\")[1].text.strip() == \"下一页\":\n        next_url = urljoin(book_url, soup.select(\".index-container-btn\")[1].get(\"href\"))\n        response = requests.get(next_url, timeout=(5, 10))\n        response.encoding = \"utf-8\"\n        soup = BeautifulSoup(response.text, 'html.parser')\n        loadChapterList(book_url, soup, chapterList)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".content\")[0].prettify()\n    # document.querySelectorAll('#next_url')[0]\n    if soup.select(\"#next_url\")[0].text.strip() == \"下一页\":\n        next_url = urljoin(chapter_url, soup.select(\"#next_url\")[0].get(\"href\"))\n        content += getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 8,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "笔仙阁",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    url = f\"https://m.bixiange.me/e/search/indexpage.php\"\n    # POST请求的数据\n    data = f'keyboard={quote(query, encoding=\"gbk\")}&show=title&classid=0'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # 发送POST请求Content-Type:\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    # log(response.request)\n    # log(response.text)\n    # 创建BeautifulSoup对象 .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0] document.querySelectorAll(\".txt-list > li\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n\n    for i, bookEl in enumerate(soup.select(\".list > .clearfix > li\")):\n        books.append({\n            \"bookTitle\": bookEl.select(\"strong > a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"strong > a\")[0].attrs[\"href\"])\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # log(response.request)\n    # log(response.text)\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\".desc > h1\")[0].text.split(\"(\")[0]\n    # 作者\n    info[\"author\"] = soup.select(\".descTip > p\")[1].text.replace(\"作者：\", \"\")\n    # 简介\n    info[\"intro\"] = soup.select(\".descInfo\")[0].text\n    # 封面\n    info[\"cover\"] = urljoin(book_url, soup.select(\".cover > img\")[0].attrs[\"src\"])\n    # 章节列表\n    info[\"chapterList\"] = []\n    loadChapterList(book_url, soup, info[\"chapterList\"])\n    return info\n\n\ndef loadChapterList(book_url, soup, chapterList):\n    log(\"加载章节目录\")\n    for el in soup.select(\".clearfix > li > a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # 创建BeautifulSoup对象\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".content\")[0].prettify()\n\n    return content\n\n\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "蚂蚁文学",
            "js": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, quote\n\nfrom log import log\n\n\ndef search(query):\n    url = \"https://www.mayiwsk.com/modules/article/search.php\"\n    data = {\n        \"searchtype\": \"articlename\",\n        \"searchkey\": query\n    }\n    response = requests.post(url, data=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list_el = soup.select(\"#nr\")\n    results = []\n\n    for book_el in book_list_el:\n        result = {\n            \"bookTitle\": book_el.select_one(\"td:nth-child(1) > a\").text,\n            \"bookUrl\": urljoin(url, book_el.select_one(\"td:nth-child(1) > a\").get(\"href\")),\n            \"bookAuthor\": book_el.select_one(\"td:nth-child(3)\").text\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select_one(\"[property='og:novel:book_name']\").get(\"content\"),\n        \"author\": soup.select_one(\"[property='og:novel:author']\").get(\"content\"),\n        \"intro\": soup.select_one(\"#intro\").prettify(),\n        \"cover\": soup.select_one(\"#fmimg > img\").get(\"src\"),\n        \"chapterList\": []\n    }\n\n    for chapter_el in reversed(soup.select(\"#list dl > *\")):\n        if chapter_el.name == \"dt\":\n            break\n        chapter = {\n            \"title\": chapter_el.select_one(\"a\").text,\n            \"url\": urljoin(url, chapter_el.select_one(\"a\").get(\"href\"))\n        }\n        book[\"chapterList\"].insert(0, chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\"#content\").prettify()\n    return content\n\n\n",
            "version": 6,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "搜读",
            "js": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlencode, urljoin\n\nfrom log import log\n\n\ndef search(query):\n    base_url = \"http://www.soduzw.com/search.html\"\n    data = {\n        \"searchtype\": \"novelname\",\n        \"searchkey\": query\n    }\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    response = requests.post(base_url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list_el = soup.select(\".Search\")\n    results = []\n\n    for book_el in book_list_el:\n        result = {\n            \"bookTitle\": book_el.select_one(\"a\").text,\n            \"bookUrl\": urljoin(base_url, book_el.select_one(\"a\")[\"href\"].replace(\"mulu_\", \"\").replace(\".html\", \"/\")),\n            \"bookAuthor\": book_el.select(\"span\")[1].text.replace(\"作者：\", \"\")\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    children = soup.select(\".Look_list_dir .chapter a\")\n    for chapter_el in children:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el[\"href\"])\n        }\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    bid = url.split(\"mulu_\")[1].split(\"/\")[0]\n    cid = url.split(\"mulu_\")[1].split(\"/\")[1].replace(\".html\", \"\")\n    data = {\n        \"bid\": bid,\n        \"cid\": cid,\n        \"siteid\": \"0\",\n        \"url\": \"\"\n    }\n    response = requests.post(\"http://www.soduzw.com/novelsearch/chapter/transcode.html\", data=data, timeout=(5, 10))\n    # log(response.request)\n    # log(response.text)\n    content = response.json()[\"info\"]\n    return content\n\n\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "香书小说",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef search(query):\n    url = f\"http://www.xbiqugu.la/modules/article/waps.php\"\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    }\n    response = requests.post(url, data={\"searchkey\": query}, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list_el = soup.select(\".grid tr\")\n    results = []\n\n    for book_el in book_list_el[1:]:\n        result = {\n            \"bookTitle\": book_el.select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, book_el.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": book_el.select(\"td\")[2].text\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"cover\": soup.select(\"meta[property='og:image']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    children = soup.select(\"#list a\")\n    for chapter_el in children:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        # http://www.xbiqugu.net/83/83137/33405352.html\n        # http://wap.xbiqugu.net/wapbook/83137_33405352.html\n        # http://wap.xbiqugu.net/wapbook/83_83137.html\n        urls = chapter[\"url\"].split(\"/\")\n        chapter[\"url\"] = f\"http://wap.xbiqugu.net/wapbook/{urls[-2]}_{urls[-1]}\"\n\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.select(\"#nr1\")[0].prettify()\n\n    a = soup.select_one(\"#pb_next\")\n    if a and \"下一页\" in a.text:\n        next_url = urljoin(url, a.get(\"href\"))\n        next_content = getChapterContent(next_url)\n        content += next_content\n\n    return content\n\n\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "得奇小说",
            "js": "from urllib.parse import urljoin, quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef search(query):\n    # https://www.deqixs.com/tag/?key=%E8%B5%A4%E5%BF%83%E5%B7%A1%E5%A4%A9\n    url = f\"https://www.deqixs.com/tag/?key={quote(query)}\"\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list = soup.select(\".item\")\n    results = []\n\n    for book_element in book_list:\n        result = {\n            \"bookTitle\": book_element.select(\"a\")[1].text,\n            \"bookUrl\": urljoin(url, book_element.select_one(\"a\").get(\"href\")),\n            \"bookAuthor\": book_element.select(\"a\")[2].text.replace(\"作者：\", \"\")\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\".item a\")[1].text,\n        \"author\": soup.select(\".item a\")[2].text.replace(\"作者：\", \"\"),\n        \"intro\": soup.select_one(\".des\").prettify(),\n        \"cover\": urljoin(url, soup.select_one(\".item a img\").get(\"src\")),\n        \"chapterList\": []\n    }\n\n    get_chapter_list(url, soup, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, soup, chapter_list):\n    chapter_list_el = soup.select(\"#list ul a\")\n\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.append(chapter)\n\n    next_page = soup.select_one(\".gr\")\n    if next_page and next_page.get(\"href\"):\n        url = urljoin(url, next_page.get(\"href\"))\n        response = requests.get(url, timeout=(5, 10))\n        response.encoding = 'utf-8'\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n        get_chapter_list(url, soup, chapter_list)\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\".con\").prettify()\n    next_url_el = soup.select(\".prenext a\")[-1]\n    if next_url_el and next_url_el.text == \"下一页\":\n        next_url = urljoin(url, next_url_el.get(\"href\"))\n        return content + \"\\n\" + getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 2,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "69书吧",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom SessionManager import start_verification_activity\nfrom log import log\n\n\ndef isSupported(url):\n    if \"69shuba.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = f\"https://www.69shuba.com/modules/article/search.php\"\n    cookies = start_verification_activity(url)\n    headers = {\n        \"Cookie\": cookies,\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    data = {\"searchkey\": query.encode('gbk'), \"searchtype\": \"all\"}\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list = soup.select(\".newbox li\")\n    results = []\n\n    for book_element in book_list:\n        results.append({\n            \"bookTitle\": book_element.select_one(\"h3\").text.strip(),\n            \"bookUrl\": urljoin(url, book_element.select_one(\".imgbox\").get(\"href\")),\n            \"bookAuthor\": book_element.select_one(\"label\").text.strip()\n        })\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"cover\": soup.select(\"meta[property='og:image']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    get_chapter_list(url, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, chapter_list):\n    url = url.replace(\".htm\", \"/\")\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\"#catalog a\")\n    for i, chapter_el in enumerate(chapter_list_el):\n        if i == 0:\n            continue\n        chapter = {\n            \"title\": chapter_el.text.strip(),\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.insert(0, chapter)\n\n\ndef getChapterContent(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    txt_el = soup.select_one(\".txtnav\")\n    # 删除h1\n    for h1 in txt_el.find_all(\"h1\"):\n        h1.decompose()\n    for h1 in txt_el.find_all(\"div\"):\n        h1.decompose()\n\n    return txt_el.prettify()\n\n\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        }
    ]
}