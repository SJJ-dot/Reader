{
    "group": "默认书源",
    "bookSource": [],
    "pySource": [
        {
            "source": "手机小说",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # POST请求的URL\n    url = 'https://www.shoujixs.net/modules/article/search.php'\n    # POST请求的数据\n    data = {'searchkey': query.encode('gbk')}\n    # 发送POST请求\n    response = requests.post(url, data=data)\n    response.encoding = 'gbk'\n    log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for c_row in soup.select(\".c_row\"):\n        books.append({\n            \"bookTitle\": c_row.select(\".c_subject\")[0].select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, c_row.select(\".c_subject\")[0].select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": c_row.select(\".c_value\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = soup.select(\"meta[property='og:url']\")[0].get(\"content\")\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    parseChapterList(book_url, soup, info[\"chapterList\"])\n\n    return info\n\n\ndef parseChapterList(book_url, soup, chapterList):\n    dt = 0\n    for el in soup.select(\"#lbks dl\")[0].children:\n        if el.name == \"dt\":\n            dt = dt + 1\n\n        elif el.name == \"dd\":\n            if dt < 2:\n                continue\n            chapterList.append({\n                \"title\": el.select(\"a\")[0].text,\n                \"url\": urljoin(book_url, el.select(\"a\")[0].get(\"href\"))\n            })\n\n    if soup.select(\".right a\")[0].has_attr(\"href\"):\n        next_url = urljoin(book_url, soup.select(\".right a\")[0].get(\"href\"))\n        response = requests.get(next_url, timeout=(5, 10))\n        response.encoding = \"gbk\"\n        soup = BeautifulSoup(response.text, 'html.parser')\n        parseChapterList(book_url, soup, chapterList)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\"#zjny\")[0].prettify()\n    return content\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "shengxuxiaoshuowang",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # POST请求的URL\n    url = 'http://www.shengxuxu.net/search.html?searchtype=novelname&searchkey='\n    # POST请求的数据\n    data = {'searchkey': query.encode('utf-8'), \"searchtype\": 'novelname'}\n    # 发送get请求\n    response = requests.get(url, params=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".librarylist > li\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\".novelname\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\".novelname\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\".info span\")[1].text.replace(\"作者：\",\"\"),\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    # document.querySelectorAll(\".dirlist\")[1].querySelectorAll(\"a\")[1664]\n    for el in soup.select(\".dirlist\")[1].select(\"a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\"#chaptercontent\")[0].prettify()\n    return content\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "天天看小说",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\n\n\n# {\n#   \"bookSourceComment\": \"由 @NPCDW 分享（#85\",\n#   \"bookSourceGroup\": \"\",\n#   \"bookSourceName\": \"天天看小说\",\n#   \"bookSourceType\": 0,\n#   \"bookSourceUrl\": \"https://cn.ttkan.co\",\n#   \"bookUrlPattern\": \"\",\n#   \"customOrder\": 23,\n#   \"enabled\": true,\n#   \"enabledCookieJar\": false,\n#   \"enabledExplore\": false,\n#   \"exploreUrl\": \"\",\n#   \"header\": \"\",\n#   \"lastUpdateTime\": 1687361913056,\n#   \"loginUi\": \"\",\n#   \"loginUrl\": \"\",\n#   \"respondTime\": 21167,\n#   \"ruleBookInfo\": {\n#     \"author\": \"//meta[@name='og:novel:author']/@content\",\n#     \"coverUrl\": \"//meta[@name='og:image']/@content\",\n#     \"downloadUrls\": \"\",\n#     \"intro\": \"//meta[@name='og:description']/@content\",\n#     \"kind\": \"//meta[@name='og:novel:category']/@content&&//meta[@name='og:novel:status']/@content\",\n#     \"lastChapter\": \"//meta[@name='og:novel:latest_chapter_name']/@content\",\n#     \"name\": \"//meta[@name='og:novel:book_name']/@content\",\n#     \"tocUrl\": \"\"\n#   },\n#   \"ruleContent\": {\n#     \"content\": \"class.content@tag.p@text\",\n#     \"nextContentUrl\": \"\",\n#     \"replaceRegex\": \"\"\n#   },\n#   \"ruleExplore\": {},\n#   \"ruleReview\": {},\n#   \"ruleSearch\": {\n#     \"author\": \"tag.li.1@text\",\n#     \"bookList\": \"class.novel_cell\",\n#     \"bookUrl\": \"tag.a.0@href\",\n#     \"coverUrl\": \"tag.amp-img.0@src\",\n#     \"intro\": \"tag.li.2@text\",\n#     \"kind\": \"\",\n#     \"lastChapter\": \"\",\n#     \"name\": \"tag.h3.0@text\"\n#   },\n#   \"ruleToc\": {\n#     \"chapterList\": \"class.full_chapters@children[0]@tag.a\",\n#     \"chapterName\": \"text\",\n#     \"chapterUrl\": \"href\",\n#     \"preUpdateJs\": \"\",\n#     \"updateTime\": \"\"\n#   },\n#   \"searchUrl\": \"/novel/search?q={{key}}\",\n#   \"weight\": 0\n# }\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # 请求的URL\n    url = 'https://cn.ttkan.co/novel/search'\n    # 请求的数据\n    data = {'q': query.encode('utf-8')}\n    # 发送get请求\n    response = requests.get(url, params=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.text)\n    #   \"ruleSearch\": {\n    #     \"author\": \"tag.li.1@text\",\n    #     \"bookList\": \"class.novel_cell\",\n    #     \"bookUrl\": \"tag.a.0@href\",\n    #     \"coverUrl\": \"tag.amp-img.0@src\",\n    #     \"intro\": \"tag.li.2@text\",\n    #     \"kind\": \"\",\n    #     \"lastChapter\": \"\",\n    #     \"name\": \"tag.h3.0@text\"\n    #   },\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".novel_cell\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\"h3\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\"li\")[1].text.replace(\"作者：\", \"\"),\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    #   \"ruleBookInfo\": {\n    #     \"author\": \"//meta[@name='og:novel:author']/@content\",\n    #     \"coverUrl\": \"//meta[@name='og:image']/@content\",\n    #     \"downloadUrls\": \"\",\n    #     \"intro\": \"//meta[@name='og:description']/@content\",\n    #     \"kind\": \"//meta[@name='og:novel:category']/@content&&//meta[@name='og:novel:status']/@content\",\n    #     \"lastChapter\": \"//meta[@name='og:novel:latest_chapter_name']/@content\",\n    #     \"name\": \"//meta[@name='og:novel:book_name']/@content\",\n    #     \"tocUrl\": \"\"\n    #   }\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[name='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[name='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[name='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[name='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    #   \"ruleToc\": {\n    #     \"chapterList\": \"class.full_chapters@children[0]@tag.a\",\n    #     \"chapterName\": \"text\",\n    #     \"chapterUrl\": \"href\",\n    #     \"preUpdateJs\": \"\",\n    #     \"updateTime\": \"\"\n    #   }\n    for el in soup.select(\".full_chapters\")[0].findChildren(\"div\")[0].findChildren(\"a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    #   \"ruleContent\": {\n    #     \"content\": \"class.content@tag.p@text\",\n    #     \"nextContentUrl\": \"\",\n    #     \"replaceRegex\": \"\"\n    #   },\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    txt = \"\"\n    for p in soup.select(\".content p\"):\n        txt = txt + \"<br>\\n\" + p.prettify()\n    return txt\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "wenquge",
            "js": "import json\n\nimport requests\n\nfrom log import log\n\nfrom Crypto.Cipher import DES3\nfrom Crypto.Util.Padding import unpad\nimport base64\nimport re\n\n\ndef decrypt(ciphertext):\n    # 3DES密钥和初始化向量\n    key = b\"ZKYm5vSUhvcG9IbXNZTG1pb2\"\n    iv = b\"01234567\"\n\n    # 使用base64解码密文\n    ciphertext = base64.b64decode(re.sub(r'(\\r\\n)|(\\n)|(\\r)', '', ciphertext))\n\n    # 创建cipher对象\n    cipher = DES3.new(key, DES3.MODE_CBC, iv=iv)\n\n    # 解密\n    plaintext = unpad(cipher.decrypt(ciphertext), DES3.block_size)\n\n    return plaintext.decode('utf-8')\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # POST请求的URL\n    url = 'http://m.nshkedu.com/search/book/result'\n    # POST请求的数据\n    data = {'kw': query.encode('utf-8'), \"is_author\": 0, \"pn\": 1}\n    headers = {\"Version-Code\": \"10000\", \"Channel\": \"mz\", \"appid\": \"wengqugexs\", \"Version-Name\": \"1.0.0\"}\n    # 发送POST请求\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.text)\n    data = decrypt(json.loads(response.text)[\"data\"])\n    # log(data)\n    # 创建BeautifulSoup对象\n    # \"bookUrl\": \"@js:\\nlet bid=parseInt(java.getString('$.book_id'))\\nlet subPath=parseInt(bid/1000)\\n\\\"http://s.nshkedu.com/api/book/detail/\\\"+subPath+\\\"/\\\"+bid+\\\".json\\\"\",\n    data = json.loads(data)[\"result\"]\n    books = []\n    for bookInfo in data:\n        bid = int(bookInfo[\"book_id\"])\n        books.append({\n            \"bookTitle\": bookInfo[\"book_name\"],\n            \"bookUrl\": f\"http://s.nshkedu.com/api/book/detail/\" + str(int(bid / 1000)) + \"/\" + str(bid) + \".json\",\n            \"bookAuthor\": bookInfo[\"author_name\"],\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = json.loads(decrypt(json.loads(response.text)[\"data\"]))[\"result\"]\n    # log(soup)\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup[\"book_name\"]\n    # 作者 作    者：黄金国巫妖\n    info[\"author\"] = soup[\"author_name\"]\n    # 简介\n    info[\"intro\"] = soup[\"book_brief\"]\n    # 封面\n    info[\"cover\"] = soup[\"book_cover\"]\n    # 章节列表\n    info[\"chapterList\"] = []\n    chapter_url = f\"http://s.nshkedu.com/api/book/chapter/\" + str(int(soup[\"book_id\"] / 1000)) + \"/\" + str(\n        soup[\"book_id\"]) + \"/list.json\"\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    data = json.loads(decrypt(json.loads(response.text)[\"data\"]))[\"result\"]\n    # log(data)\n    for chapter in data:\n        info[\"chapterList\"].append({\n            \"title\": chapter[\"chapter_name\"],\n            \"url\": f\"http://s.nshkedu.com/api/book/chapter/\" + str(int(soup[\"book_id\"] / 1000)) + \"/\" + str(\n                soup[\"book_id\"]) + \"/\" + str(chapter[\"_id\"]) + \".json\"\n        })\n\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = json.loads(decrypt(json.loads(response.text)[\"data\"]))\n    # log(soup)\n    # 章节内容 html\n    content = soup[\"content\"]\n    return content.replace(\"\\n\", \"<br>\")\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "明智屋",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    query = query.encode('utf-8')\n    # url encode\n    query = quote(query)\n    url = f'https://www.mingzw.net/mzwlist/{query}.html'\n    # 发送get请求\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request.url)\n    log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".figure-horizontal\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].attrs[\"title\"],\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\"dd\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\".novel-name\")[0].text.replace(\"《\", \"\").replace(\"》\", \"\")\n    # 作者\n    info[\"author\"] = soup.select(\".picinfo a\")[0].text\n    # 简介\n    info[\"intro\"] = soup.select(\".content\")[0].text\n    # 封面\n    info[\"cover\"] = urljoin(book_url, soup.select(\".pic img\")[0].get(\"src\"))\n    # 章节列表\n    info[\"chapterList\"] = []\n    chapterUrl = soup.select(\".view-all-btn\")[0].get(\"href\")\n    parseChapterList(book_url, urljoin(book_url, chapterUrl), info[\"chapterList\"])\n\n    return info\n\n\ndef parseChapterList(book_url, chapterUrl, chapterList):\n    response = requests.get(chapterUrl, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 递归加载章节列表\n    dt = 0\n    for el in soup.select(\".content a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(chapterUrl, el.get(\"href\"))\n        })\n\n    chapterList.pop(-1)\n    chapterList.pop(-1)\n    chapterList.pop(-1)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".contents\")[0].prettify()\n    content = content[:content.rindex(\"推荐小说:\")]\n    return content\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "书趣阁",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    # query = query.encode('utf-8')\n    # url encode\n    # query = quote(query)\n    url = f'https://www.22shuqu.com/search/'\n    # searchkey=%E6%88%91%E7%9A%84&Submit=\n    # data = {\"searchkey\": query, \"Submit\": \"\"}\n    data = f'searchkey={quote(query)}&Submit={quote(\"搜索\")}'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # 发送get请求\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for i,bookEl in enumerate(soup.select(\".txt-list > li \")):\n        if i == 0:\n            continue\n        books.append({\n            \"bookTitle\": bookEl.select(\".s2 > a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\".s2 > a\")[0].get(\"href\"))\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n\n    while True:\n\n        for el in soup.select(\".section-list\")[1].select(\"a\"):\n            info[\"chapterList\"].append({\n                \"title\": el.text,\n                \"url\": urljoin(book_url, el.get(\"href\"))\n            })\n        if soup.select(\".index-container-btn\")[1].text.strip() == \"下一页\":\n            soup = BeautifulSoup(requests.get(urljoin(book_url, soup.select(\".index-container-btn\")[1].get(\"href\")), timeout=(5, 10)).text, 'html.parser')\n        else:\n            break\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\"#content\")[0].prettify()\n\n    if soup.select(\"#next_url\")[0].text.strip() == \"下一页\":\n        content += getChapterContent(urljoin(chapter_url, soup.select(\"#next_url\")[0].get(\"href\")))\n    return content\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "起点",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    # query = query.encode('utf-8')\n    # url encode\n    query = quote(query)\n    url = f\"https://m.qidian.com/soushu/{query}.html\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    log(response.text)\n    # 创建BeautifulSoup对象 .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0]\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".y-list__item\"):\n        bid = bookEl.select(\"a\")[0].attrs[\"data-bid\"]\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].attrs[\"title\"].replace(\"在线阅读\", \"\"),\n            \"bookUrl\": f\"https://m.qidian.com/book/{bid}/\",\n            \"bookAuthor\": bookEl.select(\"div\")[2].select(\"p\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    log(response.request)\n    log(response.text)\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    log(\"加载章节目录\")\n    chapterUrl = urljoin(book_url, soup.select(\"#details-menu\")[0].attrs[\"href\"])\n    response = requests.get(chapterUrl, headers=headers, timeout=(5, 10))\n    log(response.request)\n    response.encoding = \"utf-8\"\n    log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    for el in soup.select(\".y-list__content .y-list__item a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.select(\"h2\")[0].text.replace(\"\\xa0\", \" \"),\n            \"url\": \"https://m.qidian.com\" + el.attrs[\"href\"]\n        })\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    log(response.request)\n    log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".jsChapterWrapper > div\")[0].prettify()\n    return content\n\n\n",
            "version": 7,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "22笔趣阁",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    # utf8编码\n    # query = query.encode('utf-8')\n    # url encode\n    url = f\"https://www.22biqu.com/ss/\"\n    # POST请求的数据\n    data = f'searchkey={quote(query)}&Submit={quote(\"搜索\")}'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # 发送POST请求Content-Type:\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    # 创建BeautifulSoup对象 .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0] document.querySelectorAll(\".txt-list > li\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n\n    for i, bookEl in enumerate(soup.select(\".txt-list > li\")):\n        if i == 0:\n            continue\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].attrs[\"href\"]),\n            \"bookAuthor\": bookEl.select(\".s4\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # log(response.request)\n    # log(response.text)\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # 作者\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # 简介\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # 封面\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # 章节列表\n    info[\"chapterList\"] = []\n    loadChapterList(book_url, soup, info[\"chapterList\"])\n    return info\n\n\ndef loadChapterList(book_url, soup, chapterList):\n    log(\"加载章节目录\")\n    for el in soup.select(\".section-list\")[1].select(\"a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n    if soup.select(\".index-container-btn\")[1].text.strip() == \"下一页\":\n        next_url = urljoin(book_url, soup.select(\".index-container-btn\")[1].get(\"href\"))\n        response = requests.get(next_url, timeout=(5, 10))\n        response.encoding = \"utf-8\"\n        soup = BeautifulSoup(response.text, 'html.parser')\n        loadChapterList(book_url, soup, chapterList)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # 创建BeautifulSoup对象\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".content\")[0].prettify()\n    # document.querySelectorAll('#next_url')[0]\n    if soup.select(\"#next_url\")[0].text.strip() == \"下一页\":\n        next_url = urljoin(chapter_url, soup.select(\"#next_url\")[0].get(\"href\"))\n        content += getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 8,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "笔仙阁",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\n\ndef search(query):\n    \"\"\"\n    书源搜索函数\n    :param query: 搜索关键词\n    :return [{\"bookTitle\":\"书名\", \"bookUrl\": \"书籍链接\", \"bookAuthor\": \"作者\"}, ...] 搜索结果列表\n    \"\"\"\n    url = f\"https://m.bixiange.me/e/search/indexpage.php\"\n    # POST请求的数据\n    data = f'keyboard={quote(query, encoding=\"gbk\")}&show=title&classid=0'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # 发送POST请求Content-Type:\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    # log(response.request)\n    # log(response.text)\n    # 创建BeautifulSoup对象 .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0] document.querySelectorAll(\".txt-list > li\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n\n    for i, bookEl in enumerate(soup.select(\".list > .clearfix > li\")):\n        books.append({\n            \"bookTitle\": bookEl.select(\"strong > a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"strong > a\")[0].attrs[\"href\"])\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    获取章节列表\n    :param book_url: 书籍链接\n    :return\n    {\"url\": \"书籍链接\", \"title\": \"书名\", \"author\": \"作者\", \"intro\": \"简介\", \"cover\": \"封面链接\",\n     \"chapterList\": [{\"title\": \"章节名\", \"URL\": \"章节链接\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # log(response.request)\n    # log(response.text)\n    info = {}\n    # 创建BeautifulSoup对象\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # 书名\n    info[\"title\"] = soup.select(\".desc > h1\")[0].text.split(\"(\")[0]\n    # 作者\n    info[\"author\"] = soup.select(\".descTip > p\")[1].text.replace(\"作者：\", \"\")\n    # 简介\n    info[\"intro\"] = soup.select(\".descInfo\")[0].text\n    # 封面\n    info[\"cover\"] = urljoin(book_url, soup.select(\".cover > img\")[0].attrs[\"src\"])\n    # 章节列表\n    info[\"chapterList\"] = []\n    loadChapterList(book_url, soup, info[\"chapterList\"])\n    return info\n\n\ndef loadChapterList(book_url, soup, chapterList):\n    log(\"加载章节目录\")\n    for el in soup.select(\".clearfix > li > a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    获取章节内容\n    :param chapter_url: 章节链接\n    :return: 章节内容 html格式\n    \"\"\"\n    # 发送get请求\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # 创建BeautifulSoup对象\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # 章节内容 html\n    content = soup.select(\".content\")[0].prettify()\n\n    return content\n\n\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "蚂蚁文学",
            "js": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, quote\n\nfrom log import log\n\n\ndef search(query):\n    url = \"https://www.mayiwsk.com/modules/article/search.php\"\n    data = {\n        \"searchtype\": \"articlename\",\n        \"searchkey\": query\n    }\n    response = requests.post(url, data=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list_el = soup.select(\"#nr\")\n    results = []\n\n    for book_el in book_list_el:\n        result = {\n            \"bookTitle\": book_el.select_one(\"td:nth-child(1) > a\").text,\n            \"bookUrl\": urljoin(url, book_el.select_one(\"td:nth-child(1) > a\").get(\"href\")),\n            \"bookAuthor\": book_el.select_one(\"td:nth-child(3)\").text\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select_one(\"[property='og:novel:book_name']\").get(\"content\"),\n        \"author\": soup.select_one(\"[property='og:novel:author']\").get(\"content\"),\n        \"intro\": soup.select_one(\"#intro\").prettify(),\n        \"cover\": soup.select_one(\"#fmimg > img\").get(\"src\"),\n        \"chapterList\": []\n    }\n\n    for chapter_el in reversed(soup.select(\"#list dl > *\")):\n        if chapter_el.name == \"dt\":\n            break\n        chapter = {\n            \"title\": chapter_el.select_one(\"a\").text,\n            \"url\": urljoin(url, chapter_el.select_one(\"a\").get(\"href\"))\n        }\n        book[\"chapterList\"].insert(0, chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\"#content\").prettify()\n    return content\n\n\n",
            "version": 6,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "c笔趣阁",
            "js": "import re\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, quote\n\nfrom log import log\n\n\ndef search(query):\n    base_url = \"http://www.changshengrui.com/\"\n    url = f\"{base_url}search/?searchkey={quote(query)}\"\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list = soup.select(\".item\")\n    results = []\n\n    for book_element in book_list:\n        result = {\n            \"bookTitle\": book_element.select_one(\"dl > dt > a\").text,\n            \"bookUrl\": urljoin(base_url, book_element.select_one(\"dl > dt > a\").get(\"href\")),\n            \"bookAuthor\": \"\".join(book_element.select_one(\".btm\").find_all(string=True, recursive=False)).strip()\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select_one(\"#info > h1\").text,\n        \"author\": soup.select_one(\"#info > p:nth-child(2) > a\").text,\n        \"intro\": soup.select_one(\"#intro\").prettify(),\n        \"cover\": urljoin(url, soup.select_one(\"#fmimg > img\").get(\"data-original\")),\n        \"chapterList\": []\n    }\n\n    chapter_url = urljoin(url, soup.select_one(\"#maininfo .chapterlist\").get(\"href\"))\n    get_chapter_list(chapter_url, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, chapter_list):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\"#list > dl > a\")\n\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.append(chapter)\n\n    next_page = soup.select_one(\".right a\")\n    if next_page and next_page.text == \"下一页\":\n        get_chapter_list(urljoin(url, next_page.get(\"href\")), chapter_list)\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\"#booktxt\").prettify()\n    next_url_el = soup.select(\".next_url\")[0]\n    if next_url_el and next_url_el.text == \"下一页\":\n        next_url = re.search(r\"const\\snext_page\\s=\\s'(.+)';\", html).group(1)\n        return content + \"\\n\" + getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 2,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "四桃小说",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef search(query):\n    url = f\"https://www.4txs.com/search.html\"\n    response = requests.post(url, data={\"searchkey\": query}, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list = soup.select(\".library > li\")\n    results = []\n\n    for book_element in book_list:\n        result = {\n            \"bookTitle\": book_element.select_one(\".bookname\").text,\n            \"bookUrl\": urljoin(url, book_element.select_one(\".bookname\").get(\"href\")),\n            \"bookAuthor\": \"\".join(book_element.select_one(\".author\").get(\"href\")).strip()\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\"url\": url,\n            \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n            \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n            \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n            \"cover\": soup.select(\".detail .bookimg img\")[0].get(\"src\"),\n            \"chapterList\": []}\n\n    chapter_url = urljoin(url, soup.select_one(\".detail .action a\").get(\"href\"))\n    get_chapter_list(chapter_url, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, chapter_list):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\".read > dl\")[1].select(\"a\")\n\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.append(chapter)\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\"#content\").prettify()\n    next_url_el = soup.select(\".page a\")[2]\n    if next_url_el and \"下一页\" in next_url_el.text:\n        next_url = urljoin(url, next_url_el.get(\"href\"))\n        return content + \"\\n\" + getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 1,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "搜读",
            "js": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlencode, urljoin\n\nfrom log import log\n\n\ndef search(query):\n    base_url = \"http://www.soduzw.com/search.html\"\n    data = {\n        \"searchtype\": \"novelname\",\n        \"searchkey\": query\n    }\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    response = requests.post(base_url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list_el = soup.select(\".Search\")\n    results = []\n\n    for book_el in book_list_el:\n        result = {\n            \"bookTitle\": book_el.select_one(\"a\").text,\n            \"bookUrl\": urljoin(base_url, book_el.select_one(\"a\")[\"href\"].replace(\"mulu_\", \"\").replace(\".html\", \"/\")),\n            \"bookAuthor\": book_el.select(\"span\")[1].text.replace(\"作者：\", \"\")\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    children = soup.select(\".Look_list_dir .chapter a\")\n    for chapter_el in children:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el[\"href\"])\n        }\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    bid = url.split(\"mulu_\")[1].split(\"/\")[0]\n    cid = url.split(\"mulu_\")[1].split(\"/\")[1].replace(\".html\", \"\")\n    data = {\n        \"bid\": bid,\n        \"cid\": cid,\n        \"siteid\": \"0\",\n        \"url\": \"\"\n    }\n    response = requests.post(\"http://www.soduzw.com/novelsearch/chapter/transcode.html\", data=data, timeout=(5, 10))\n    # log(response.request)\n    # log(response.text)\n    content = response.json()[\"info\"]\n    return content\n\n\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "香书小说",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef search(query):\n    url = f\"http://www.xbiqugu.net/modules/article/waps.php\"\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    }\n    response = requests.post(url, data={\"searchkey\": query}, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list_el = soup.select(\".grid tr\")\n    results = []\n\n    for book_el in book_list_el[1:]:\n        result = {\n            \"bookTitle\": book_el.select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, book_el.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": book_el.select(\"td\")[2].text\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"cover\": soup.select(\"meta[property='og:image']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    children = soup.select(\"#list a\")\n    for chapter_el in children:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        # http://www.xbiqugu.net/83/83137/33405352.html\n        # http://wap.xbiqugu.net/wapbook/83137_33405352.html\n        # http://wap.xbiqugu.net/wapbook/83_83137.html\n        urls = chapter[\"url\"].split(\"/\")\n        chapter[\"url\"] = f\"http://wap.xbiqugu.net/wapbook/{urls[-2]}_{urls[-1]}\"\n\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.select(\"#nr1\")[0].prettify()\n\n    a = soup.select_one(\"#pb_next\")\n    if a and \"下一页\" in a.text:\n        next_url = urljoin(url, a.get(\"href\"))\n        next_content = getChapterContent(next_url)\n        content += next_content\n\n    return content\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "得奇小说",
            "js": "from urllib.parse import urljoin, quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef search(query):\n    # https://www.deqixs.com/tag/?key=%E8%B5%A4%E5%BF%83%E5%B7%A1%E5%A4%A9\n    url = f\"https://www.deqixs.com/tag/?key={quote(query)}\"\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list = soup.select(\".item\")\n    results = []\n\n    for book_element in book_list:\n        result = {\n            \"bookTitle\": book_element.select(\"a\")[1].text,\n            \"bookUrl\": urljoin(url, book_element.select_one(\"a\").get(\"href\")),\n            \"bookAuthor\": book_element.select(\"a\")[2].text.replace(\"作者：\", \"\")\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\".item a\")[1].text,\n        \"author\": soup.select(\".item a\")[2].text.replace(\"作者：\", \"\"),\n        \"intro\": soup.select_one(\".des\").prettify(),\n        \"cover\": urljoin(url, soup.select_one(\".item a img\").get(\"src\")),\n        \"chapterList\": []\n    }\n\n    get_chapter_list(url, soup, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, soup, chapter_list):\n    chapter_list_el = soup.select(\"#list ul a\")\n\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.append(chapter)\n\n    next_page = soup.select_one(\".gr\")\n    if next_page and next_page.get(\"href\"):\n        url = urljoin(url, next_page.get(\"href\"))\n        response = requests.get(url, timeout=(5, 10))\n        response.encoding = 'utf-8'\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n        get_chapter_list(url, soup, chapter_list)\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\".con\").prettify()\n    next_url_el = soup.select(\".prenext a\")[-1]\n    if next_url_el and next_url_el.text == \"下一页\":\n        next_url = urljoin(url, next_url_el.get(\"href\"))\n        return content + \"\\n\" + getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 2,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "69书吧",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom SessionManager import start_verification_activity\nfrom log import log\n\n\ndef isSupported(url):\n    if \"69shuba.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = f\"https://www.69shuba.com/modules/article/search.php\"\n    cookies = start_verification_activity(url)\n    headers = {\n        \"Cookie\": cookies,\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    data = {\"searchkey\": query.encode('gbk'), \"searchtype\": \"all\"}\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list = soup.select(\".newbox li\")\n    results = []\n\n    for book_element in book_list:\n        results.append({\n            \"bookTitle\": book_element.select_one(\"h3\").text.strip(),\n            \"bookUrl\": urljoin(url, book_element.select_one(\".imgbox\").get(\"href\")),\n            \"bookAuthor\": book_element.select_one(\"label\").text.strip()\n        })\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"cover\": soup.select(\"meta[property='og:image']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    get_chapter_list(url, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, chapter_list):\n    url = url.replace(\".htm\", \"/\")\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\"#catalog a\")\n    for i, chapter_el in enumerate(chapter_list_el):\n        if i == 0:\n            continue\n        chapter = {\n            \"title\": chapter_el.text.strip(),\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.insert(0, chapter)\n\n\ndef getChapterContent(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    txt_el = soup.select_one(\".txtnav\")\n    # 删除h1\n    for h1 in txt_el.find_all(\"h1\"):\n        h1.decompose()\n    for h1 in txt_el.find_all(\"div\"):\n        h1.decompose()\n\n    return txt_el.prettify()\n\n\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        }
    ]
}