{
    "group": "ÈªòËÆ§‰π¶Ê∫ê",
    "bookSource": [],
    "pySource": [
        {
            "source": "shengxuxiaoshuowang",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\n\ndef isSupported(url):\n    if \"shengxuxu.net\" in url:\n        return True\n    return False\n\ndef search(query):\n    \"\"\"\n    ‰π¶Ê∫êÊêúÁ¥¢ÂáΩÊï∞\n    :param query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØç\n    :return [{\"bookTitle\":\"‰π¶Âêç\", \"bookUrl\": \"‰π¶Á±çÈìæÊé•\", \"bookAuthor\": \"‰ΩúËÄÖ\"}, ...] ÊêúÁ¥¢ÁªìÊûúÂàóË°®\n    \"\"\"\n    # POSTËØ∑Ê±ÇÁöÑURL\n    url = 'http://www.shengxuxu.net/search.html?searchtype=novelname&searchkey='\n    # POSTËØ∑Ê±ÇÁöÑÊï∞ÊçÆ\n    data = {'searchkey': query.encode('utf-8'), \"searchtype\": 'novelname'}\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(url, params=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.text)\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".librarylist > li\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\".novelname\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\".novelname\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\".info span\")[1].text.replace(\"‰ΩúËÄÖÔºö\",\"\"),\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂàóË°®\n    :param book_url: ‰π¶Á±çÈìæÊé•\n    :return\n    {\"url\": \"‰π¶Á±çÈìæÊé•\", \"title\": \"‰π¶Âêç\", \"author\": \"‰ΩúËÄÖ\", \"intro\": \"ÁÆÄ‰ªã\", \"cover\": \"Â∞ÅÈù¢ÈìæÊé•\",\n     \"chapterList\": [{\"title\": \"Á´†ËäÇÂêç\", \"URL\": \"Á´†ËäÇÈìæÊé•\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # ‰π¶Âêç\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # ‰ΩúËÄÖ\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # ÁÆÄ‰ªã\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # Â∞ÅÈù¢\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # Á´†ËäÇÂàóË°®\n    info[\"chapterList\"] = []\n    # document.querySelectorAll(\".dirlist\")[1].querySelectorAll(\"a\")[1664]\n    for el in soup.select(\".dirlist\")[1].select(\"a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂÜÖÂÆπ\n    :param chapter_url: Á´†ËäÇÈìæÊé•\n    :return: Á´†ËäÇÂÜÖÂÆπ htmlÊ†ºÂºè\n    \"\"\"\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Á´†ËäÇÂÜÖÂÆπ html\n    content = soup.select(\"#chaptercontent\")[0].prettify()\n    return content\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "ÊòéÊô∫Â±ã",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\ndef isSupported(url):\n    if \"mingzw.net\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    \"\"\"\n    ‰π¶Ê∫êÊêúÁ¥¢ÂáΩÊï∞\n    :param query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØç\n    :return [{\"bookTitle\":\"‰π¶Âêç\", \"bookUrl\": \"‰π¶Á±çÈìæÊé•\", \"bookAuthor\": \"‰ΩúËÄÖ\"}, ...] ÊêúÁ¥¢ÁªìÊûúÂàóË°®\n    \"\"\"\n    # utf8ÁºñÁ†Å\n    query = query.encode('utf-8')\n    # url encode\n    query = quote(query)\n    url = f'https://www.mingzw.net/mzwlist/{query}.html'\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request.url)\n    log(response.text)\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".figure-horizontal\"):\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].attrs[\"title\"],\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": bookEl.select(\"dd\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂàóË°®\n    :param book_url: ‰π¶Á±çÈìæÊé•\n    :return\n    {\"url\": \"‰π¶Á±çÈìæÊé•\", \"title\": \"‰π¶Âêç\", \"author\": \"‰ΩúËÄÖ\", \"intro\": \"ÁÆÄ‰ªã\", \"cover\": \"Â∞ÅÈù¢ÈìæÊé•\",\n     \"chapterList\": [{\"title\": \"Á´†ËäÇÂêç\", \"URL\": \"Á´†ËäÇÈìæÊé•\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # ‰π¶Âêç\n    info[\"title\"] = soup.select(\".novel-name\")[0].text.replace(\"„Ää\", \"\").replace(\"„Äã\", \"\")\n    # ‰ΩúËÄÖ\n    info[\"author\"] = soup.select(\".picinfo a\")[0].text\n    # ÁÆÄ‰ªã\n    info[\"intro\"] = soup.select(\".content\")[0].text\n    # Â∞ÅÈù¢\n    info[\"cover\"] = urljoin(book_url, soup.select(\".pic img\")[0].get(\"src\"))\n    # Á´†ËäÇÂàóË°®\n    info[\"chapterList\"] = []\n    chapterUrl = soup.select(\".view-all-btn\")[0].get(\"href\")\n    parseChapterList(book_url, urljoin(book_url, chapterUrl), info[\"chapterList\"])\n\n    return info\n\n\ndef parseChapterList(book_url, chapterUrl, chapterList):\n    response = requests.get(chapterUrl, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # ÈÄíÂΩíÂä†ËΩΩÁ´†ËäÇÂàóË°®\n    dt = 0\n    for el in soup.select(\".content a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(chapterUrl, el.get(\"href\"))\n        })\n\n    chapterList.pop(-1)\n    chapterList.pop(-1)\n    chapterList.pop(-1)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂÜÖÂÆπ\n    :param chapter_url: Á´†ËäÇÈìæÊé•\n    :return: Á´†ËäÇÂÜÖÂÆπ htmlÊ†ºÂºè\n    \"\"\"\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Á´†ËäÇÂÜÖÂÆπ html\n    content = soup.select(\".contents\")[0].prettify()\n    content = content[:content.rindex(\"Êé®ËçêÂ∞èËØ¥:\")]\n    return content\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "‰π¶Ë∂£ÈòÅ",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\ndef isSupported(url):\n    if \"22shuqu.com\" in url:\n        return True\n    return False\n\ndef search(query):\n    \"\"\"\n    ‰π¶Ê∫êÊêúÁ¥¢ÂáΩÊï∞\n    :param query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØç\n    :return [{\"bookTitle\":\"‰π¶Âêç\", \"bookUrl\": \"‰π¶Á±çÈìæÊé•\", \"bookAuthor\": \"‰ΩúËÄÖ\"}, ...] ÊêúÁ¥¢ÁªìÊûúÂàóË°®\n    \"\"\"\n    # utf8ÁºñÁ†Å\n    # query = query.encode('utf-8')\n    # url encode\n    # query = quote(query)\n    url = f'https://www.22shuqu.com/search/'\n    # searchkey=%E6%88%91%E7%9A%84&Submit=\n    # data = {\"searchkey\": query, \"Submit\": \"\"}\n    data = f'searchkey={quote(query)}&Submit={quote(\"ÊêúÁ¥¢\")}'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for i,bookEl in enumerate(soup.select(\".txt-list > li \")):\n        if i == 0:\n            continue\n        books.append({\n            \"bookTitle\": bookEl.select(\".s2 > a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\".s2 > a\")[0].get(\"href\"))\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂàóË°®\n    :param book_url: ‰π¶Á±çÈìæÊé•\n    :return\n    {\"url\": \"‰π¶Á±çÈìæÊé•\", \"title\": \"‰π¶Âêç\", \"author\": \"‰ΩúËÄÖ\", \"intro\": \"ÁÆÄ‰ªã\", \"cover\": \"Â∞ÅÈù¢ÈìæÊé•\",\n     \"chapterList\": [{\"title\": \"Á´†ËäÇÂêç\", \"URL\": \"Á´†ËäÇÈìæÊé•\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(book_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    info = {}\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # ‰π¶Âêç\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # ‰ΩúËÄÖ\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # ÁÆÄ‰ªã\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # Â∞ÅÈù¢\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # Á´†ËäÇÂàóË°®\n    info[\"chapterList\"] = []\n\n    while True:\n\n        for el in soup.select(\".section-list\")[1].select(\"a\"):\n            info[\"chapterList\"].append({\n                \"title\": el.text,\n                \"url\": urljoin(book_url, el.get(\"href\"))\n            })\n        if soup.select(\".index-container-btn\")[1].text.strip() == \"‰∏ã‰∏ÄÈ°µ\":\n            soup = BeautifulSoup(requests.get(urljoin(book_url, soup.select(\".index-container-btn\")[1].get(\"href\")), timeout=(5, 10)).text, 'html.parser')\n        else:\n            break\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂÜÖÂÆπ\n    :param chapter_url: Á´†ËäÇÈìæÊé•\n    :return: Á´†ËäÇÂÜÖÂÆπ htmlÊ†ºÂºè\n    \"\"\"\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    response = requests.get(chapter_url, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Á´†ËäÇÂÜÖÂÆπ html\n    content = soup.select(\"#content\")[0].prettify()\n\n    if soup.select(\"#next_url\")[0].text.strip() == \"‰∏ã‰∏ÄÈ°µ\":\n        content += getChapterContent(urljoin(chapter_url, soup.select(\"#next_url\")[0].get(\"href\")))\n    return content\n\n\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "Ëµ∑ÁÇπ",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\ndef isSupported(url):\n    if \"qidian.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    \"\"\"\n    ‰π¶Ê∫êÊêúÁ¥¢ÂáΩÊï∞\n    :param query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØç\n    :return [{\"bookTitle\":\"‰π¶Âêç\", \"bookUrl\": \"‰π¶Á±çÈìæÊé•\", \"bookAuthor\": \"‰ΩúËÄÖ\"}, ...] ÊêúÁ¥¢ÁªìÊûúÂàóË°®\n    \"\"\"\n    # utf8ÁºñÁ†Å\n    # query = query.encode('utf-8')\n    # url encode\n    query = quote(query)\n    url = f\"https://m.qidian.com/soushu/{query}.html\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    log(response.text)\n    # ÂàõÂª∫BeautifulSoupÂØπË±° .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0]\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n    for bookEl in soup.select(\".y-list__item\"):\n        bid = bookEl.select(\"a\")[0].attrs[\"data-bid\"]\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].attrs[\"title\"].replace(\"Âú®Á∫øÈòÖËØª\", \"\"),\n            \"bookUrl\": f\"https://m.qidian.com/book/{bid}/\",\n            \"bookAuthor\": bookEl.select(\"div\")[2].select(\"p\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂàóË°®\n    :param book_url: ‰π¶Á±çÈìæÊé•\n    :return\n    {\"url\": \"‰π¶Á±çÈìæÊé•\", \"title\": \"‰π¶Âêç\", \"author\": \"‰ΩúËÄÖ\", \"intro\": \"ÁÆÄ‰ªã\", \"cover\": \"Â∞ÅÈù¢ÈìæÊé•\",\n     \"chapterList\": [{\"title\": \"Á´†ËäÇÂêç\", \"URL\": \"Á´†ËäÇÈìæÊé•\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    log(response.request)\n    log(response.text)\n    info = {}\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # ‰π¶Âêç\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # ‰ΩúËÄÖ\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # ÁÆÄ‰ªã\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # Â∞ÅÈù¢\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # Á´†ËäÇÂàóË°®\n    info[\"chapterList\"] = []\n    log(\"Âä†ËΩΩÁ´†ËäÇÁõÆÂΩï\")\n    chapterUrl = urljoin(book_url, soup.select(\"#details-menu\")[0].attrs[\"href\"])\n    response = requests.get(chapterUrl, headers=headers, timeout=(5, 10))\n    log(response.request)\n    response.encoding = \"utf-8\"\n    log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    for el in soup.select(\".y-list__content .y-list__item a\"):\n        info[\"chapterList\"].append({\n            \"title\": el.select(\"h2\")[0].text.replace(\"\\xa0\", \" \"),\n            \"url\": \"https://m.qidian.com\" + el.attrs[\"href\"]\n        })\n    return info\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂÜÖÂÆπ\n    :param chapter_url: Á´†ËäÇÈìæÊé•\n    :return: Á´†ËäÇÂÜÖÂÆπ htmlÊ†ºÂºè\n    \"\"\"\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    log(response.request)\n    log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Á´†ËäÇÂÜÖÂÆπ html\n    content = soup.select(\".jsChapterWrapper > div\")[0].prettify()\n    return content\n\n\n",
            "version": 8,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "22Á¨îË∂£ÈòÅ",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\ndef isSupported(url):\n    if \"22biqu.com\" in url:\n        return True\n    return False\n\ndef search(query):\n    \"\"\"\n    ‰π¶Ê∫êÊêúÁ¥¢ÂáΩÊï∞\n    :param query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØç\n    :return [{\"bookTitle\":\"‰π¶Âêç\", \"bookUrl\": \"‰π¶Á±çÈìæÊé•\", \"bookAuthor\": \"‰ΩúËÄÖ\"}, ...] ÊêúÁ¥¢ÁªìÊûúÂàóË°®\n    \"\"\"\n    # utf8ÁºñÁ†Å\n    # query = query.encode('utf-8')\n    # url encode\n    url = f\"https://www.22biqu.com/ss/\"\n    # POSTËØ∑Ê±ÇÁöÑÊï∞ÊçÆ\n    data = f'searchkey={quote(query)}&Submit={quote(\"ÊêúÁ¥¢\")}'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # ÂèëÈÄÅPOSTËØ∑Ê±ÇContent-Type:\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    # ÂàõÂª∫BeautifulSoupÂØπË±° .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0] document.querySelectorAll(\".txt-list > li\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n\n    for i, bookEl in enumerate(soup.select(\".txt-list > li\")):\n        if i == 0:\n            continue\n        books.append({\n            \"bookTitle\": bookEl.select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"a\")[0].attrs[\"href\"]),\n            \"bookAuthor\": bookEl.select(\".s4\")[0].text,\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂàóË°®\n    :param book_url: ‰π¶Á±çÈìæÊé•\n    :return\n    {\"url\": \"‰π¶Á±çÈìæÊé•\", \"title\": \"‰π¶Âêç\", \"author\": \"‰ΩúËÄÖ\", \"intro\": \"ÁÆÄ‰ªã\", \"cover\": \"Â∞ÅÈù¢ÈìæÊé•\",\n     \"chapterList\": [{\"title\": \"Á´†ËäÇÂêç\", \"URL\": \"Á´†ËäÇÈìæÊé•\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # log(response.request)\n    # log(response.text)\n    info = {}\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # ‰π¶Âêç\n    info[\"title\"] = soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\")\n    # ‰ΩúËÄÖ\n    info[\"author\"] = soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\")\n    # ÁÆÄ‰ªã\n    info[\"intro\"] = soup.select(\"meta[property='og:description']\")[0].get(\"content\")\n    # Â∞ÅÈù¢\n    info[\"cover\"] = soup.select(\"meta[property='og:image']\")[0].get(\"content\")\n    # Á´†ËäÇÂàóË°®\n    info[\"chapterList\"] = []\n    loadChapterList(book_url, soup, info[\"chapterList\"])\n    return info\n\n\ndef loadChapterList(book_url, soup, chapterList):\n    log(\"Âä†ËΩΩÁ´†ËäÇÁõÆÂΩï\")\n    for el in soup.select(\".section-list\")[1].select(\"a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n    if soup.select(\".index-container-btn\")[1].text.strip() == \"‰∏ã‰∏ÄÈ°µ\":\n        next_url = urljoin(book_url, soup.select(\".index-container-btn\")[1].get(\"href\"))\n        response = requests.get(next_url, timeout=(5, 10))\n        response.encoding = \"utf-8\"\n        soup = BeautifulSoup(response.text, 'html.parser')\n        loadChapterList(book_url, soup, chapterList)\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂÜÖÂÆπ\n    :param chapter_url: Á´†ËäÇÈìæÊé•\n    :return: Á´†ËäÇÂÜÖÂÆπ htmlÊ†ºÂºè\n    \"\"\"\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"utf-8\"\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Á´†ËäÇÂÜÖÂÆπ html\n    content = soup.select(\".content\")[0].prettify()\n    # document.querySelectorAll('#next_url')[0]\n    if soup.select(\"#next_url\")[0].text.strip() == \"‰∏ã‰∏ÄÈ°µ\":\n        next_url = urljoin(chapter_url, soup.select(\"#next_url\")[0].get(\"href\"))\n        content += getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 9,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "Á¨î‰ªôÈòÅ",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom log import log\nfrom urllib.parse import quote\n\ndef isSupported(url):\n    if \"bixiange.me\" in url:\n        return True\n    return False\n\ndef search(query):\n    \"\"\"\n    ‰π¶Ê∫êÊêúÁ¥¢ÂáΩÊï∞\n    :param query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØç\n    :return [{\"bookTitle\":\"‰π¶Âêç\", \"bookUrl\": \"‰π¶Á±çÈìæÊé•\", \"bookAuthor\": \"‰ΩúËÄÖ\"}, ...] ÊêúÁ¥¢ÁªìÊûúÂàóË°®\n    \"\"\"\n    url = f\"https://m.bixiange.me/e/search/indexpage.php\"\n    # POSTËØ∑Ê±ÇÁöÑÊï∞ÊçÆ\n    data = f'keyboard={quote(query, encoding=\"gbk\")}&show=title&classid=0'\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    # ÂèëÈÄÅPOSTËØ∑Ê±ÇContent-Type:\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    # log(response.request)\n    # log(response.text)\n    # ÂàõÂª∫BeautifulSoupÂØπË±° .querySelectorAll(\"div\")[2].querySelectorAll(\"p\")[0] document.querySelectorAll(\".txt-list > li\")\n    soup = BeautifulSoup(response.text, 'html.parser')\n    books = []\n\n    for i, bookEl in enumerate(soup.select(\".list > .clearfix > li\")):\n        books.append({\n            \"bookTitle\": bookEl.select(\"strong > a\")[0].text,\n            \"bookUrl\": urljoin(url, bookEl.select(\"strong > a\")[0].attrs[\"href\"])\n        })\n\n    return books\n\n\ndef getDetails(book_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂàóË°®\n    :param book_url: ‰π¶Á±çÈìæÊé•\n    :return\n    {\"url\": \"‰π¶Á±çÈìæÊé•\", \"title\": \"‰π¶Âêç\", \"author\": \"‰ΩúËÄÖ\", \"intro\": \"ÁÆÄ‰ªã\", \"cover\": \"Â∞ÅÈù¢ÈìæÊé•\",\n     \"chapterList\": [{\"title\": \"Á´†ËäÇÂêç\", \"URL\": \"Á´†ËäÇÈìæÊé•\"}, ...]\n     }\n    \"\"\"\n\n    # url book_url\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(book_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # log(response.request)\n    # log(response.text)\n    info = {}\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    soup = BeautifulSoup(response.text, 'html.parser')\n    info[\"url\"] = book_url\n    # ‰π¶Âêç\n    info[\"title\"] = soup.select(\".desc > h1\")[0].text.split(\"(\")[0]\n    # ‰ΩúËÄÖ\n    info[\"author\"] = soup.select(\".descTip > p\")[1].text.replace(\"‰ΩúËÄÖÔºö\", \"\")\n    # ÁÆÄ‰ªã\n    info[\"intro\"] = soup.select(\".descInfo\")[0].text\n    # Â∞ÅÈù¢\n    info[\"cover\"] = urljoin(book_url, soup.select(\".cover > img\")[0].attrs[\"src\"])\n    # Á´†ËäÇÂàóË°®\n    info[\"chapterList\"] = []\n    loadChapterList(book_url, soup, info[\"chapterList\"])\n    return info\n\n\ndef loadChapterList(book_url, soup, chapterList):\n    log(\"Âä†ËΩΩÁ´†ËäÇÁõÆÂΩï\")\n    for el in soup.select(\".clearfix > li > a\"):\n        chapterList.append({\n            \"title\": el.text,\n            \"url\": urljoin(book_url, el.get(\"href\"))\n        })\n\n\ndef getChapterContent(chapter_url):\n    \"\"\"\n    Ëé∑ÂèñÁ´†ËäÇÂÜÖÂÆπ\n    :param chapter_url: Á´†ËäÇÈìæÊé•\n    :return: Á´†ËäÇÂÜÖÂÆπ htmlÊ†ºÂºè\n    \"\"\"\n    # ÂèëÈÄÅgetËØ∑Ê±Ç\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36\",\n    }\n    response = requests.get(chapter_url, headers=headers, timeout=(5, 10))\n    response.encoding = \"gbk\"\n    # ÂàõÂª∫BeautifulSoupÂØπË±°\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Á´†ËäÇÂÜÖÂÆπ html\n    content = soup.select(\".content\")[0].prettify()\n\n    return content\n\n\n\n\n",
            "version": 4,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "ËöÇËöÅÊñáÂ≠¶",
            "js": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, quote\n\nfrom log import log\n\ndef isSupported(url):\n    if \"mayiwsk.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = \"https://www.mayiwsk.com/modules/article/search.php\"\n    data = {\n        \"searchtype\": \"articlename\",\n        \"searchkey\": query\n    }\n    response = requests.post(url, data=data, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list_el = soup.select(\"#nr\")\n    results = []\n\n    for book_el in book_list_el:\n        result = {\n            \"bookTitle\": book_el.select_one(\"td:nth-child(1) > a\").text,\n            \"bookUrl\": urljoin(url, book_el.select_one(\"td:nth-child(1) > a\").get(\"href\")),\n            \"bookAuthor\": book_el.select_one(\"td:nth-child(3)\").text\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select_one(\"[property='og:novel:book_name']\").get(\"content\"),\n        \"author\": soup.select_one(\"[property='og:novel:author']\").get(\"content\"),\n        \"intro\": soup.select_one(\"#intro\").prettify(),\n        \"cover\": soup.select_one(\"#fmimg > img\").get(\"src\"),\n        \"chapterList\": []\n    }\n\n    for chapter_el in reversed(soup.select(\"#list dl > *\")):\n        if chapter_el.name == \"dt\":\n            break\n        chapter = {\n            \"title\": chapter_el.select_one(\"a\").text,\n            \"url\": urljoin(url, chapter_el.select_one(\"a\").get(\"href\"))\n        }\n        book[\"chapterList\"].insert(0, chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\"#content\").prettify()\n    return content\n\n\n",
            "version": 7,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "ÊêúËØª",
            "js": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlencode, urljoin\n\nfrom log import log\n\ndef isSupported(url):\n    if \"soduzw.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    base_url = \"http://www.soduzw.com/search.html\"\n    data = {\n        \"searchtype\": \"novelname\",\n        \"searchkey\": query\n    }\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    response = requests.post(base_url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list_el = soup.select(\".Search\")\n    results = []\n\n    for book_el in book_list_el:\n        result = {\n            \"bookTitle\": book_el.select_one(\"a\").text,\n            \"bookUrl\": urljoin(base_url, book_el.select_one(\"a\")[\"href\"].replace(\"mulu_\", \"\").replace(\".html\", \"/\")),\n            \"bookAuthor\": book_el.select(\"span\")[1].text.replace(\"‰ΩúËÄÖÔºö\", \"\")\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    children = soup.select(\".Look_list_dir .chapter a\")\n    for chapter_el in children:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el[\"href\"])\n        }\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    bid = url.split(\"mulu_\")[1].split(\"/\")[0]\n    cid = url.split(\"mulu_\")[1].split(\"/\")[1].replace(\".html\", \"\")\n    data = {\n        \"bid\": bid,\n        \"cid\": cid,\n        \"siteid\": \"0\",\n        \"url\": \"\"\n    }\n    response = requests.post(\"http://www.soduzw.com/novelsearch/chapter/transcode.html\", data=data, timeout=(5, 10))\n    # log(response.request)\n    # log(response.text)\n    content = response.json()[\"info\"]\n    return content\n\n\n",
            "version": 6,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "È¶ô‰π¶Â∞èËØ¥",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef isSupported(url):\n    if \"xbiqugu.la\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = f\"http://www.xbiqugu.la/modules/article/waps.php\"\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    }\n    response = requests.post(url, data={\"searchkey\": query}, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    # log(response.request)\n    # log(response.text)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book_list_el = soup.select(\".grid tr\")\n    results = []\n\n    for book_el in book_list_el[1:]:\n        result = {\n            \"bookTitle\": book_el.select(\"a\")[0].text,\n            \"bookUrl\": urljoin(url, book_el.select(\"a\")[0].get(\"href\")),\n            \"bookAuthor\": book_el.select(\"td\")[2].text\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"cover\": soup.select(\"meta[property='og:image']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    children = soup.select(\"#list a\")\n    for chapter_el in children:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        # http://www.xbiqugu.net/83/83137/33405352.html\n        # http://wap.xbiqugu.net/wapbook/83137_33405352.html\n        # http://wap.xbiqugu.net/wapbook/83_83137.html\n        urls = chapter[\"url\"].split(\"/\")\n        chapter[\"url\"] = f\"http://wap.xbiqugu.net/wapbook/{urls[-2]}_{urls[-1]}\"\n\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.select(\"#nr1\")[0].prettify()\n\n    a = soup.select_one(\"#pb_next\")\n    if a and \"‰∏ã‰∏ÄÈ°µ\" in a.text:\n        next_url = urljoin(url, a.get(\"href\"))\n        next_content = getChapterContent(next_url)\n        content += next_content\n\n    return content\n\n\n",
            "version": 5,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "ÂæóÂ•áÂ∞èËØ¥",
            "js": "from urllib.parse import urljoin, quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\ndef isSupported(url):\n    if \"deqixs.com\" in url:\n        return True\n    return False\n\ndef search(query):\n    # https://www.deqixs.com/tag/?key=%E8%B5%A4%E5%BF%83%E5%B7%A1%E5%A4%A9\n    url = f\"https://www.deqixs.com/tag/?key={quote(query)}\"\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book_list = soup.select(\".item\")\n    results = []\n\n    for book_element in book_list:\n        result = {\n            \"bookTitle\": book_element.select(\"a\")[1].text,\n            \"bookUrl\": urljoin(url, book_element.select_one(\"a\").get(\"href\")),\n            \"bookAuthor\": book_element.select(\"a\")[2].text.replace(\"‰ΩúËÄÖÔºö\", \"\")\n        }\n        results.append(result)\n\n    return results\n\n\ndef getDetails(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\".item a\")[1].text,\n        \"author\": soup.select(\".item a\")[2].text.replace(\"‰ΩúËÄÖÔºö\", \"\"),\n        \"intro\": soup.select_one(\".des\").prettify(),\n        \"cover\": urljoin(url, soup.select_one(\".item a img\").get(\"src\")),\n        \"chapterList\": []\n    }\n\n    get_chapter_list(url, soup, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, soup, chapter_list):\n    chapter_list_el = soup.select(\"#list ul a\")\n\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text,\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.append(chapter)\n\n    next_page = soup.select_one(\".gr\")\n    if next_page and next_page.get(\"href\"):\n        url = urljoin(url, next_page.get(\"href\"))\n        response = requests.get(url, timeout=(5, 10))\n        response.encoding = 'utf-8'\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n        get_chapter_list(url, soup, chapter_list)\n\n\ndef getChapterContent(url):\n    response = requests.get(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.select_one(\".con\").prettify()\n    next_url_el = soup.select(\".prenext a\")[-1]\n    if next_url_el and next_url_el.text == \"‰∏ã‰∏ÄÈ°µ\":\n        next_url = urljoin(url, next_url_el.get(\"href\"))\n        return content + \"\\n\" + getChapterContent(next_url)\n\n    return content\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "69‰π¶Âêß",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom SessionManager import start_verification_activity, get_cookie\nfrom log import log\n\n\ndef isSupported(url):\n    if \"69shuba.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = f\"https://www.69shuba.com/modules/article/search.php\"\n    data = {\"searchkey\": query.encode('gbk'), \"searchtype\": \"all\"}\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n\n    html_text = \"\"\n    cookies = get_cookie(url)\n    if \"cf_clearance\" in cookies:\n        headers[\"Cookie\"] = cookies\n\n        response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n        response.encoding = 'gbk'\n        log(response.request)\n        log(response)\n        html_text = response.text\n\n    if \"cf_clearance\" not in cookies or \"newbox\" not in html_text:\n        cookies = start_verification_activity(url)\n        headers[\"Cookie\"] = cookies\n        response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n        response.encoding = 'gbk'\n        log(response.request)\n        log(response)\n        html_text = response.text\n\n    soup = BeautifulSoup(html_text, 'html.parser')\n    book_list = soup.select(\".newbox li\")\n    results = []\n\n    for book_element in book_list:\n        results.append({\n            \"bookTitle\": book_element.select_one(\"h3\").text.strip(),\n            \"bookUrl\": urljoin(url, book_element.select_one(\".imgbox\").get(\"href\")),\n            \"bookAuthor\": book_element.select_one(\"label\").text.strip()\n        })\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\"meta[property='og:novel:book_name']\")[0].get(\"content\"),\n        \"author\": soup.select(\"meta[property='og:novel:author']\")[0].get(\"content\"),\n        \"intro\": soup.select(\"meta[property='og:description']\")[0].get(\"content\"),\n        \"cover\": soup.select(\"meta[property='og:image']\")[0].get(\"content\"),\n        \"chapterList\": []\n    }\n\n    get_chapter_list(url, book[\"chapterList\"])\n\n    return book\n\n\ndef get_chapter_list(url, chapter_list):\n    url = url.replace(\".htm\", \"/\")\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\"#catalog a\")\n    for i, chapter_el in enumerate(chapter_list_el):\n        if i == 0:\n            continue\n        chapter = {\n            \"title\": chapter_el.text.strip(),\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        chapter_list.insert(0, chapter)\n\n\ndef getChapterContent(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    txt_el = soup.select_one(\".txtnav\")\n    # Âà†Èô§h1\n    for h1 in txt_el.find_all(\"h1\"):\n        h1.decompose()\n    for h1 in txt_el.find_all(\"div\"):\n        h1.decompose()\n\n    return txt_el.prettify()\n\n\n",
            "version": 6,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "Â¶ôÂë≥‰π¶Â±ã",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef isSupported(url):\n    if \"twinfoo.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    results = []\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf8'\n    html = response.text\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select(\".row h1\")[0].get_text(strip=True),\n        \"author\": \"‰ΩöÂêç\",\n        \"intro\": \"\",\n        \"chapterList\": []\n    }\n    title = book[\"title\"]\n    title_p1 = book[\"title\"].split(\"(\")[0].strip()\n    if title_p1:\n        book[\"title\"] = title_p1\n\n    chapter_list_el = soup.select(\".row\")[4].select(\".col-md-4 a\")\n    for i, chapter_el in enumerate(chapter_list_el):\n        chapter = {\n            \"title\": chapter_el.text.replace(title, \"\").replace(title_p1, \"\").replace(\"(ÂÆåÊï¥Áâà)\", \"\").strip(),\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf8'\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    page_a = soup.select(\"#page-links a\")\n    text = soup.select_one(\".blurstxt\").prettify()\n    for a in page_a:\n        response = requests.get(urljoin(url, a.get(\"href\")), headers=headers, timeout=(5, 10))\n        response.encoding = 'utf8'\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n        text += \"<p></p>\"\n        text += soup.select_one(\".blurstxt\").prettify()\n\n    return text\n\n\n",
            "version": 3,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "È£òÂ§©ü™ú",
            "js": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef isSupported(url):\n    if \"piaotia.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = \"https://www.piaotia.com/modules/article/search.php\"\n    data = {\"searchkey\": query.encode('gbk'), \"searchtype\": \"articlename\"}\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.post(url, data=data, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    results = []\n    if \"bookinfo\" in response.request.url:\n        results.append({\n            \"bookTitle\": soup.select_one(\"#content h1\").text.strip(),\n            \"bookUrl\": response.request.url,\n            \"bookAuthor\": soup.select(\"td\")[5].text.split(\"ËÄÖÔºö\")[1].strip()\n        })\n    else:\n        book_list = soup.select(\"tr\")[1:]  # Skip the header row\n        for book_element in book_list:\n            results.append({\n                \"bookTitle\": book_element.select_one(\"a\").text.strip(),\n                \"bookUrl\": urljoin(url, book_element.select_one(\"a\").get(\"href\")),\n                \"bookAuthor\": book_element.select(\"td\")[2].text.strip()\n            })\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select_one(\"#content h1\").text.strip(),\n        \"author\": soup.select(\"td\")[5].text.split(\"ËÄÖÔºö\")[1].strip(),\n        \"intro\": \"ÂÜÖÂÆπÁÆÄ‰ªãÔºö\",\n        \"chapterList\": []\n    }\n    span_element = soup.find('span', string=\"ÂÜÖÂÆπÁÆÄ‰ªãÔºö\")\n    if span_element:\n        siblings = span_element.find_next_siblings(string=True)\n        for sibling in siblings:\n            if sibling.strip():\n                book[\"intro\"] += \"\\n\"\n                book[\"intro\"] += sibling.strip()\n                break\n    # https://www.piaotia.com/bookinfo/15/15621.html\n    # https://www.piaotia.com/html/15/15621/\n\n    response = requests.get(url.replace(\"bookinfo\", \"html\").replace(\".html\", \"/\"), headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    html = response.text\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\".centent a\")\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text.strip(),\n            \"url\": urljoin(response.request.url, chapter_el.get(\"href\"))\n        }\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'gbk'\n    log(response)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    content = soup.find_all('br')[1].prettify()\n    content = content.split(\"<!--\")[0]\n    return content\n\n\n",
            "version": 2,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        },
        {
            "source": "Á¨îË∂£ÈòÅbv",
            "js": "from urllib.parse import urljoin, quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom log import log\n\n\ndef isSupported(url):\n    if \"bvquge.com\" in url:\n        return True\n    return False\n\n\ndef search(query):\n    url = \"https://www.bvquge.com/so/\" + quote(query)\n    response = requests.post(url, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    results = []\n    book_list = soup.select(\".item\")\n    for book_element in book_list:\n        results.append({\n            \"bookTitle\": book_element.select(\"a\")[1].text.strip(),\n            \"bookUrl\": urljoin(url, book_element.select(\"a\")[1].get(\"href\")),\n            \"bookAuthor\": book_element.select(\"a\")[2].text.replace(\"‰ΩúËÄÖÔºö\", \"\").strip()\n        })\n\n    return results\n\n\ndef getDetails(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'uft-8'\n    html = response.text\n    log(response.request)\n    log(response)\n    soup = BeautifulSoup(html, 'html.parser')\n    book = {\n        \"url\": url,\n        \"title\": soup.select_one(\".booktxt h1\").text.strip(),\n        \"author\": soup.select_one(\".booktxt a\").text.split(\"ËÄÖÔºö\")[1].strip(),\n        \"intro\": soup.select_one(\".des\").text.strip(),\n        \"chapterList\": []\n    }\n\n    soup = BeautifulSoup(html, 'html.parser')\n    chapter_list_el = soup.select(\"#list a\")\n    for chapter_el in chapter_list_el:\n        chapter = {\n            \"title\": chapter_el.text.strip(),\n            \"url\": urljoin(url, chapter_el.get(\"href\"))\n        }\n        book[\"chapterList\"].append(chapter)\n\n    return book\n\n\ndef getChapterContent(url):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n    }\n    response = requests.get(url, headers=headers, timeout=(5, 10))\n    response.encoding = 'utf-8'\n    log(response)\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n    con = soup.select_one(\".con\")\n    if con and con.h1:\n        con.h1.decompose()\n    content = con.prettify() if con else \"\"\n    return content\n\n\n",
            "version": 1,
            "original": false,
            "enable": true,
            "requestDelay": -1,
            "website": ""
        }
    ]
}